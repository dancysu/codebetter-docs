# 本地部署 DeepSeek-R1



## （1）使用 Ollama 部署 Deepseek

#### 步骤 1：安装 ollama

[Ollama](https://ollama.com/) 是一个开源的本地大语言模型运行框架，专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计。进入官网后，点击 `Download` 按钮下载 `Ollama` 客户端，然后进行安装。

![ollama-001](/images/ai/ollama-001.png)

#### 步骤 2：下载/运行模型

安装完 `Ollama` 后，我们到 `Ollama` 官网的模型页面(https://ollama.com/search)挑选一下模型。

![ollama-002](/images/ai/ollama-002.png)

这里我们选择 `deepseek-r1`

![ollama-003](/images/ai/ollama-003.png)

如果你还没下载过这个模型执行以下命令后就会自动下载，如果已经下载过它就会运行这个模型。

```bash
ollama run deepseek-r1:latest
```

运行后，你就可以在终端和大模型对话了。

![ollama-004](/images/ai/ollama-004.png)

:::details 停止或卸载模型

如果需要停止或卸载模型，可以使用以下命令

```bash
ollama stop deepseek-r1:latest
```

:::

## （2）使用 Docker 部署 Dify

#### 步骤 1：安装docker

打开 [Docker](https://www.docker.com/) 官网，根据你系统下载对应的安装包，然后还是傻瓜式安装即可。

![docker-001](/images/ai/docker-001.png)

#### 步骤 2：Github 下载 dify

[Dify](https://github.com/langgenius/dify) 是一个开源的大语言模型（LLM）应用开发平台，旨在帮助开发者快速构建和部署生成式 AI 应用程序。

![dify-001](/images/ai/dify-001.png)

#### 步骤 3：启动 dify

```bash
cd dify-main

# 配置环境变量
cd docker/
cp .env.example .env

# docker compose 启动
docker compose up -d
```



## （3）使用 Dify 接入 Ollame 模型

#### 步骤 1：dify初始化

启动成功后，访问 [http://localhost/install](http://localhost/install) 

（1）注册管理员账户

![dify-002](/images/ai/dify-002.png)

（2）使用管理员账户进行登录

![dify-003](/images/ai/dify-003.png)

（3）登录成功后，可以看到以下页面

![dify-004](/images/ai/dify-004.png)

#### 步骤 2：添加 ollama 模型

在配置工作流之前，我们需要给 `Dify` 配置大语言模型。

（1）点击页面右上角的管理员头像，然后选择“设置”。

![dify-005](/images/ai/dify-005.png)

（2）选择“模型供应商”，然后点击“Ollama”的卡片添加模型。

![dify-006](/images/ai/dify-006.png)

（3）参数配置

- `模型名称`：填入 `deepseek-r1:latest`。
- `基础 URL`：填入 `http://host.docker.internal:11434`。

![dify-007](/images/ai/dify-007.png)

#### 步骤 3：搭建工作流

（1）创建空白应用

![dify-008](/images/ai/dify-008.png)

（2）选择“工作流”

![dify-009](/images/ai/dify-009.png)

（3）配置工作流

![dify-010](/images/ai/dify-010.png)
